{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f84393-98d8-4928-bfd3-946134db87ec",
   "metadata": {},
   "source": [
    "# Central and Worker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56693fe6-164d-43d0-becb-a1b0e3762b01",
   "metadata": {},
   "source": [
    "This notebook goes over the necessery code for central and worker federated learning agents, which have their own machine learning pipelines that enable the following incremental actions:\n",
    "1. Global model initilization in central\n",
    "2. Sending initial model to workers\n",
    "3. Training a new model in workers\n",
    "4. Returning model updates to central\n",
    "5. Aggregating updates into a global model\n",
    "6. Repeating steps 2 to 4 until model converges\n",
    "\n",
    "In this project we will use the [Synthetic Financial Datasets For Fraud Detection](https://www.kaggle.com/datasets/ealaxi/paysim1/data) to simulate a fraud detection infrastucture, where the central node is controlled by the trade organization and worker nodes are different banks that belong to that organisation where the trade organisation decides to use federated learning to facilitate a adapting, robust and private fraud detection system for their partners.The import we will use in this notebook are the following:\n",
    "\n",
    "- Pandas\n",
    "- Numpy\n",
    "- Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792434d7-8475-45a2-805c-0618e8b4ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20437/2162656668.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96556a9-52d2-4f62-be2d-4bd859ee6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_df = pd.read_csv('data/Fraud_Detection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbd8f72-cc5b-4595-a851-1de6525a070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>9839.64</td>\n",
       "      <td>C1231006815</td>\n",
       "      <td>170136.00</td>\n",
       "      <td>160296.36</td>\n",
       "      <td>M1979787155</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>1864.28</td>\n",
       "      <td>C1666544295</td>\n",
       "      <td>21249.00</td>\n",
       "      <td>19384.72</td>\n",
       "      <td>M2044282225</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C1305486145</td>\n",
       "      <td>181.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C553264065</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C840083671</td>\n",
       "      <td>181.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C38997010</td>\n",
       "      <td>21182.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11668.14</td>\n",
       "      <td>C2048537720</td>\n",
       "      <td>41554.00</td>\n",
       "      <td>29885.86</td>\n",
       "      <td>M1230701703</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362615</th>\n",
       "      <td>743</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>C786484425</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C776919290</td>\n",
       "      <td>0.00</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362616</th>\n",
       "      <td>743</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>C1529008245</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1881841831</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362617</th>\n",
       "      <td>743</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>C1162922333</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1365125890</td>\n",
       "      <td>68488.84</td>\n",
       "      <td>6379898.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362618</th>\n",
       "      <td>743</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>C1685995037</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C2080388513</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362619</th>\n",
       "      <td>743</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>C1280323807</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C873221189</td>\n",
       "      <td>6510099.11</td>\n",
       "      <td>7360101.63</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6362620 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         step      type      amount     nameOrig  oldbalanceOrg  \\\n",
       "0           1   PAYMENT     9839.64  C1231006815      170136.00   \n",
       "1           1   PAYMENT     1864.28  C1666544295       21249.00   \n",
       "2           1  TRANSFER      181.00  C1305486145         181.00   \n",
       "3           1  CASH_OUT      181.00   C840083671         181.00   \n",
       "4           1   PAYMENT    11668.14  C2048537720       41554.00   \n",
       "...       ...       ...         ...          ...            ...   \n",
       "6362615   743  CASH_OUT   339682.13   C786484425      339682.13   \n",
       "6362616   743  TRANSFER  6311409.28  C1529008245     6311409.28   \n",
       "6362617   743  CASH_OUT  6311409.28  C1162922333     6311409.28   \n",
       "6362618   743  TRANSFER   850002.52  C1685995037      850002.52   \n",
       "6362619   743  CASH_OUT   850002.52  C1280323807      850002.52   \n",
       "\n",
       "         newbalanceOrig     nameDest  oldbalanceDest  newbalanceDest  isFraud  \\\n",
       "0             160296.36  M1979787155            0.00            0.00        0   \n",
       "1              19384.72  M2044282225            0.00            0.00        0   \n",
       "2                  0.00   C553264065            0.00            0.00        1   \n",
       "3                  0.00    C38997010        21182.00            0.00        1   \n",
       "4              29885.86  M1230701703            0.00            0.00        0   \n",
       "...                 ...          ...             ...             ...      ...   \n",
       "6362615            0.00   C776919290            0.00       339682.13        1   \n",
       "6362616            0.00  C1881841831            0.00            0.00        1   \n",
       "6362617            0.00  C1365125890        68488.84      6379898.11        1   \n",
       "6362618            0.00  C2080388513            0.00            0.00        1   \n",
       "6362619            0.00   C873221189      6510099.11      7360101.63        1   \n",
       "\n",
       "         isFlaggedFraud  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "...                 ...  \n",
       "6362615               0  \n",
       "6362616               0  \n",
       "6362617               0  \n",
       "6362618               0  \n",
       "6362619               0  \n",
       "\n",
       "[6362620 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6a03b-08c7-492b-957e-fe5e736b9bdb",
   "metadata": {},
   "source": [
    "## Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1467d6-eb8d-4daa-a2a1-60ee7152fa90",
   "metadata": {},
   "source": [
    "The columns are:\n",
    "- Row index = The amount of logs\n",
    "- Step = One hour in the real world \n",
    "- Type = Transaction type: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER\n",
    "- Amount = Unit of local currency\n",
    "- NameOrig = Customer who started the transaction\n",
    "- OldbalanceOrig = Initial balance before the transaction\n",
    "- NewbalanceOrig = New balance after the transaction\n",
    "- NameDest = Customer who is the recipient of the transaction\n",
    "- oldbalanceDest = Initial balance recipient before the transaction.\n",
    "- NewbalanceDest = New balance recipient after the transaction\n",
    "- IsFraud = The transactions made by the fraudulent agents.\n",
    "- IsFlaggedFraud = Existing detection, where more than 200.000 transcations are flagged\n",
    "\n",
    "In order to simulate fraud detection, we need to remove the following columns:\n",
    "- OldbalanceOrg\n",
    "- NewbalanceOrig\n",
    "- OldbalanceDest\n",
    "- NewbalanceDest\n",
    "- IsFlaggedFraud (Should be used for comparison, but not for training a model)\n",
    "\n",
    "After that, we need to modify the following columns:\n",
    "- type = Requires hot one encoding using integers\n",
    "- nameOrig = requires string integer encoding\n",
    "- nameDest = requires string integer encoding\n",
    "- amount = round up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f8ca605-206d-4848-8a70-b33ae875124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting(\n",
    "    source_df: any\n",
    ") -> any:\n",
    "    print('Formatting data')\n",
    "    formated_df = source_df.copy()\n",
    "    \n",
    "    irrelevant_columns = [\n",
    "        'oldbalanceOrg',\n",
    "        'newbalanceOrig',\n",
    "        'oldbalanceDest',\n",
    "        'newbalanceDest'\n",
    "    ]\n",
    "    formated_df.drop(\n",
    "        columns = irrelevant_columns, \n",
    "        inplace = True\n",
    "    )\n",
    "    print('Columns dropped')\n",
    "    formated_df = pd.get_dummies(\n",
    "        data = formated_df, \n",
    "        columns = ['type']\n",
    "    )\n",
    "    \n",
    "    for column in formated_df.columns:\n",
    "        if 'type' in column:\n",
    "            formated_df[column] = formated_df[column].astype(int)\n",
    "    print('One hot coded type')\n",
    "\n",
    "    unique_values_orig = formated_df['nameOrig'].unique()\n",
    "    unique_values_dest = formated_df['nameDest'].unique()\n",
    "    \n",
    "    unique_value_list_orig = unique_values_orig.tolist()\n",
    "    unique_value_list_dest = unique_values_dest.tolist()\n",
    "\n",
    "    print('Orig amount:', len(unique_value_list_orig))\n",
    "    print('Dest amount:', len(unique_value_list_dest))\n",
    "    \n",
    "    set_orig_ids = set(unique_value_list_orig)\n",
    "    set_dest_ids = set(unique_value_list_dest)\n",
    "    intersection = set_dest_ids.intersection(set_orig_ids)\n",
    "\n",
    "    print('Orig and Dest duplicates', len(intersection))\n",
    "    \n",
    "    set_dest_ids.difference_update(intersection)\n",
    "    fixed_unique_value_list_dest = list(set_dest_ids)\n",
    "    print('Fixed Dest amount:',len(fixed_unique_value_list_dest))\n",
    "    \n",
    "    orig_encoding_dict = {}\n",
    "    index = 1\n",
    "    for string in unique_value_list_orig:\n",
    "        if not string in orig_encoding_dict:\n",
    "            orig_encoding_dict[string] = index\n",
    "            index = index + 1\n",
    "\n",
    "    dest_encoding_dict = {}\n",
    "    cont_index = len(orig_encoding_dict) + 1\n",
    "    for string in fixed_unique_value_list_dest:\n",
    "        if not string in dest_encoding_dict:\n",
    "            dest_encoding_dict[string] = cont_index\n",
    "            cont_index = cont_index + 1\n",
    "    print('Orig dict amount:', len(orig_encoding_dict))\n",
    "    print('Dest dict amount:', len(dest_encoding_dict))\n",
    "    \n",
    "    print('Orig and dest string-integer encodings created')\n",
    "\n",
    "    string_orig_values = formated_df['nameOrig'].tolist()\n",
    "    string_dest_values = formated_df['nameDest'].tolist()\n",
    "\n",
    "    orig_encoded_values = []\n",
    "    for string in string_orig_values:\n",
    "        orig_encoded_values.append(orig_encoding_dict[string])\n",
    "\n",
    "    dest_encoded_values = []\n",
    "    for string in string_dest_values:\n",
    "        if not string in dest_encoding_dict:\n",
    "            dest_encoded_values.append(orig_encoding_dict[string])\n",
    "            continue\n",
    "        dest_encoded_values.append(dest_encoding_dict[string])\n",
    "\n",
    "    formated_df['nameOrig'] = orig_encoded_values\n",
    "    formated_df['nameDest'] = dest_encoded_values\n",
    "\n",
    "    print('Orig encoded values amount:', len(orig_encoded_values))\n",
    "    print('Dest encoded values amount:', len(dest_encoded_values))\n",
    "    \n",
    "    print('Orig and dest encodings set')\n",
    "\n",
    "    formated_df['amount'] = formated_df['amount'].round(0).astype(int)\n",
    "    print('Amount rounded')\n",
    "\n",
    "    column_order = [\n",
    "        'step',\n",
    "        'amount',\n",
    "        'nameOrig',\n",
    "        'nameDest',\n",
    "        'type_CASH_IN',\n",
    "        'type_CASH_OUT',\n",
    "        'type_DEBIT',\n",
    "        'type_PAYMENT',\n",
    "        'type_TRANSFER',\n",
    "        'isFraud',\n",
    "        'isFlaggedFraud'\n",
    "    ]\n",
    "    formated_df = formated_df[column_order]\n",
    "    print('Columns reordered')\n",
    "    print('Dataframe shape:', formated_df.shape)\n",
    "    print('Formatting done')\n",
    "    return formated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12445ad2-b554-4bc4-a187-37f67c2e0715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting data\n",
      "Columns dropped\n",
      "One hot coded type\n",
      "Orig amount: 6353307\n",
      "Dest amount: 2722362\n",
      "Orig and Dest duplicates 1769\n",
      "Fixed Dest amount: 2720593\n",
      "Orig dict amount: 6353307\n",
      "Dest dict amount: 2720593\n",
      "Orig and dest string-integer encodings created\n",
      "Orig encoded values amount: 6362620\n",
      "Dest encoded values amount: 6362620\n",
      "Orig and dest encodings set\n",
      "Amount rounded\n",
      "Columns reordered\n",
      "Dataframe shape: (6362620, 11)\n",
      "Formatting done\n"
     ]
    }
   ],
   "source": [
    "formated_data_df = formatting(\n",
    "    source_df = source_data_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ac16fca-ac1d-4ab4-b7dc-156d1972d4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>type_CASH_IN</th>\n",
       "      <th>type_CASH_OUT</th>\n",
       "      <th>type_DEBIT</th>\n",
       "      <th>type_PAYMENT</th>\n",
       "      <th>type_TRANSFER</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9840</td>\n",
       "      <td>1</td>\n",
       "      <td>6788653</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1864</td>\n",
       "      <td>2</td>\n",
       "      <td>6647762</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>3</td>\n",
       "      <td>6405410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>4</td>\n",
       "      <td>7291669</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>11668</td>\n",
       "      <td>5</td>\n",
       "      <td>8220099</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362615</th>\n",
       "      <td>743</td>\n",
       "      <td>339682</td>\n",
       "      <td>6353303</td>\n",
       "      <td>8111677</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362616</th>\n",
       "      <td>743</td>\n",
       "      <td>6311409</td>\n",
       "      <td>6353304</td>\n",
       "      <td>8024143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362617</th>\n",
       "      <td>743</td>\n",
       "      <td>6311409</td>\n",
       "      <td>6353305</td>\n",
       "      <td>7595045</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362618</th>\n",
       "      <td>743</td>\n",
       "      <td>850003</td>\n",
       "      <td>6353306</td>\n",
       "      <td>7587114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362619</th>\n",
       "      <td>743</td>\n",
       "      <td>850003</td>\n",
       "      <td>6353307</td>\n",
       "      <td>6746168</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6362620 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         step   amount  nameOrig  nameDest  type_CASH_IN  type_CASH_OUT  \\\n",
       "0           1     9840         1   6788653             0              0   \n",
       "1           1     1864         2   6647762             0              0   \n",
       "2           1      181         3   6405410             0              0   \n",
       "3           1      181         4   7291669             0              1   \n",
       "4           1    11668         5   8220099             0              0   \n",
       "...       ...      ...       ...       ...           ...            ...   \n",
       "6362615   743   339682   6353303   8111677             0              1   \n",
       "6362616   743  6311409   6353304   8024143             0              0   \n",
       "6362617   743  6311409   6353305   7595045             0              1   \n",
       "6362618   743   850003   6353306   7587114             0              0   \n",
       "6362619   743   850003   6353307   6746168             0              1   \n",
       "\n",
       "         type_DEBIT  type_PAYMENT  type_TRANSFER  isFraud  isFlaggedFraud  \n",
       "0                 0             1              0        0               0  \n",
       "1                 0             1              0        0               0  \n",
       "2                 0             0              1        1               0  \n",
       "3                 0             0              0        1               0  \n",
       "4                 0             1              0        0               0  \n",
       "...             ...           ...            ...      ...             ...  \n",
       "6362615           0             0              0        1               0  \n",
       "6362616           0             0              1        1               0  \n",
       "6362617           0             0              0        1               0  \n",
       "6362618           0             0              1        1               0  \n",
       "6362619           0             0              0        1               0  \n",
       "\n",
       "[6362620 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formated_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fb80c03-27c7-4e22-a13a-09a6e21362a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_data_df.to_csv('data/Formated_Fraud_Detection_Data.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8a8305-37c6-4b49-9679-99568471033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Formated_Fraud_Detection_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e836ef-3cd0-4063-8478-80683f1d3093",
   "metadata": {},
   "source": [
    "## Regular Learning with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087b3f4-efeb-4d92-87c6-e73beaa11443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "987054b4-1b62-4d76-8019-77ea927be9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def preprocess_into_tensors(\n",
    "    data_path: str,\n",
    "    used_columns: list,\n",
    "    rows: int,\n",
    "    scaled_columns: list,\n",
    "    target_column: str,\n",
    "    set_seed: int\n",
    ") -> any:\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    preprocessed_df = df[used_columns]\n",
    "\n",
    "    preprocessed_df = preprocessed_df[:rows]\n",
    "\n",
    "    for column in scaled_columns:\n",
    "        mean = preprocessed_df[column].mean()\n",
    "        std_dev = preprocessed_df[column].std()\n",
    "        preprocessed_df[column] = (preprocessed_df[column] - mean)/std_dev\n",
    "\n",
    "    X = preprocessed_df.drop(target_column, axis = 1).values\n",
    "    y = preprocessed_df[target_column].values\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, \n",
    "        y, \n",
    "        test_size = 0.2, \n",
    "        random_state = set_seed\n",
    "    )\n",
    "\n",
    "    print('X train:',X_train.shape)\n",
    "    print('X test:',X_test.shape)\n",
    "    print('Y train:',y_train.shape)\n",
    "    print('Y test:',y_test.shape)\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.int32)\n",
    "    y_test = np.array(y_test, dtype=np.int32)\n",
    "    \n",
    "    train_tensor = TensorDataset(\n",
    "        torch.tensor(X_train), \n",
    "        torch.tensor(y_train, dtype=torch.float32)\n",
    "    )\n",
    "    test_tensor = TensorDataset(\n",
    "        torch.tensor(X_test), \n",
    "        torch.tensor(y_test, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "    return X_train.shape[1], train_tensor, test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4aa2328-0a47-499f-aded-3d68226049b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.linear = nn.Linear(dim, 1, bias=bias)\n",
    "        self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x).view(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def train_step(model, batch):\n",
    "        x, y = batch\n",
    "        out = model(x)\n",
    "        loss = model.loss(out, y)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def test_step(model, batch):\n",
    "        x, y = batch\n",
    "        out = model(x)\n",
    "        loss = model.loss(out, y)\n",
    "        preds = out > 0 # Predict y = 1 if P(y = 1) > 0.5\n",
    "        corrects = torch.tensor(torch.sum(preds == y).item())\n",
    "        return loss, corrects\n",
    "\n",
    "def get_loaders(\n",
    "    set_seed: int,\n",
    "    sample_rate: float,\n",
    "    train_tensor: any,\n",
    "    test_tensor: any\n",
    ") -> any:\n",
    "    train_loader = DataLoader(\n",
    "        train_tensor,\n",
    "        batch_size=int(len(train_tensor) * sample_rate),\n",
    "        generator=torch.Generator().manual_seed(set_seed)\n",
    "    )\n",
    "    test_loader = DataLoader(test_tensor, 64)\n",
    "    return train_loader,test_loader\n",
    "\n",
    "def train(\n",
    "    model: any, \n",
    "    train_loader: any, \n",
    "    opt_func: any, \n",
    "    learning_rate: float, \n",
    "    num_epochs: int,  \n",
    "    random_seed: int, \n",
    "    verbose = True\n",
    ") -> int:\n",
    "    optimizer = opt_func(model.parameters(), learning_rate)\n",
    "    model_type = type(model)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model_type.train_step(model, batch)\n",
    "            loss.backward()\n",
    "            losses.append(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Epoch {}, loss = {}\".format(epoch + 1, torch.sum(loss) / len(train_loader)))\n",
    "    \n",
    "def test(\n",
    "    model: any, \n",
    "    test_loader: any\n",
    ") -> any:\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        total_size = 0\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            total_size += len(batch[1])\n",
    "            loss, corrects = model.test_step(model, batch)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(corrects)\n",
    "\n",
    "        average_loss = np.array(loss).sum() / total_size\n",
    "        total_accuracy = np.array(accuracies).sum() / total_size\n",
    "        return average_loss, total_accuracy\n",
    "\n",
    "def run_model_pipeline(\n",
    "    set_seed: int,\n",
    "    learning_rate: float,\n",
    "    sample_rate: float,\n",
    "    num_epochs: int,\n",
    "    input_dim: int,\n",
    "    train_tensor: any,\n",
    "    test_tensor: any\n",
    ") -> any:\n",
    "    torch.manual_seed(set_seed)\n",
    "    print('Loaders')\n",
    "    given_train_loader, given_test_loader = get_loaders(\n",
    "        set_seed,\n",
    "        sample_rate,\n",
    "        train_tensor,\n",
    "        test_tensor\n",
    "    )\n",
    "    print('Model')\n",
    "    lr_model = LogisticRegression(dim = input_dim)\n",
    "    print('Train')\n",
    "    train(\n",
    "        model = lr_model, \n",
    "        train_loader = given_train_loader, \n",
    "        opt_func = torch.optim.SGD, \n",
    "        learning_rate = learning_rate, \n",
    "        num_epochs = num_epochs,  \n",
    "        random_seed = set_seed, \n",
    "        verbose = True\n",
    "    )\n",
    "    \n",
    "    print('Test')\n",
    "    average_loss, total_accuracy = test(\n",
    "        model = lr_model, \n",
    "        test_loader = given_test_loader\n",
    "    )\n",
    "    print('Complete')\n",
    "    return average_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef994480-f5e7-498b-b657-06ad1b78f809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (8000, 6)\n",
      "X test: (2000, 6)\n",
      "Y train: (8000,)\n",
      "Y test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "input_dim, train_tensor, test_tensor = preprocess_into_tensors(\n",
    "    data_path = 'data/Formated_Fraud_Detection_Data.csv',\n",
    "    used_columns = [\n",
    "        'amount',\n",
    "        'type_CASH_IN',\n",
    "        'type_CASH_OUT',\n",
    "        'type_DEBIT',\n",
    "        'type_PAYMENT',\n",
    "        'type_TRANSFER',\n",
    "        'isFraud'\n",
    "    ],\n",
    "    rows = 10000,\n",
    "    scaled_columns = [\n",
    "        'amount'\n",
    "    ],\n",
    "    target_column = 'isFraud',\n",
    "    set_seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54aeac6d-d711-408c-a856-b4b35dfb6b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders\n",
      "Model\n",
      "Train\n",
      "Epoch 1, loss = 0.005733905825763941\n",
      "Epoch 2, loss = 0.005491575691848993\n",
      "Epoch 3, loss = 0.005264477338641882\n",
      "Epoch 4, loss = 0.005051491782069206\n",
      "Epoch 5, loss = 0.004851583391427994\n",
      "Test\n",
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00022987823188304901, 0.908)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_pipeline(\n",
    "    set_seed = 42,\n",
    "    learning_rate = 0.001,\n",
    "    sample_rate = 0.01,\n",
    "    num_epochs = 5,\n",
    "    input_dim = input_dim,\n",
    "    train_tensor = train_tensor,\n",
    "    test_tensor = test_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9f44e-269c-4ce5-99a4-704d82167408",
   "metadata": {},
   "source": [
    "## Federated Learning with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea4729c-6c9e-47af-a094-0e0c5b1fb272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6293/3294056286.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6687d53c-443d-4030-9f8c-3492f1cac524",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def preprocess_into_train_and_test_tensors(\n",
    "    data_path: str,\n",
    "    used_columns: list,\n",
    "    start_row: int,\n",
    "    end_row: int,\n",
    "    scaled_columns: list,\n",
    "    target_column: str,\n",
    "    set_seed: int\n",
    ") -> any:\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    preprocessed_df = df[used_columns]\n",
    "\n",
    "    preprocessed_df = preprocessed_df[start_row:end_row]\n",
    "\n",
    "    for column in scaled_columns:\n",
    "        mean = preprocessed_df[column].mean()\n",
    "        std_dev = preprocessed_df[column].std()\n",
    "        preprocessed_df[column] = (preprocessed_df[column] - mean)/std_dev\n",
    "\n",
    "    X = preprocessed_df.drop(target_column, axis = 1).values\n",
    "    y = preprocessed_df[target_column].values\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, \n",
    "        y, \n",
    "        test_size = 0.2, \n",
    "        random_state = set_seed\n",
    "    )\n",
    "\n",
    "    print('X train:',X_train.shape)\n",
    "    print('X test:',X_test.shape)\n",
    "    print('Y train:',y_train.shape)\n",
    "    print('Y test:',y_test.shape)\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.int32)\n",
    "    y_test = np.array(y_test, dtype=np.int32)\n",
    "    \n",
    "    train_tensor = TensorDataset(\n",
    "        torch.tensor(X_train), \n",
    "        torch.tensor(y_train, dtype=torch.float32)\n",
    "    )\n",
    "    test_tensor = TensorDataset(\n",
    "        torch.tensor(X_test), \n",
    "        torch.tensor(y_test, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "    return X_train.shape[0], X_train.shape[1], train_tensor, test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395f96d0-5f6d-4820-928d-f70b8c04b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedLogisticRegression(nn.Module):\n",
    "    def __init__(self, dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.linear = nn.Linear(dim, 1, bias=bias)\n",
    "        self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x).view(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def train_step(model, batch):\n",
    "        x, y = batch\n",
    "        out = model(x)\n",
    "        loss = model.loss(out, y)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def test_step(model, batch):\n",
    "        x, y = batch\n",
    "        out = model(x)\n",
    "        loss = model.loss(out, y)\n",
    "        preds = out > 0 # Predict y = 1 if P(y = 1) > 0.5\n",
    "        corrects = torch.tensor(torch.sum(preds == y).item())\n",
    "        return loss, corrects\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parameters(model):\n",
    "        return model.state_dict()\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_parameters(model, parameters):\n",
    "        model.load_state_dict(parameters)\n",
    "\n",
    "def get_loaders(\n",
    "    set_seed: int,\n",
    "    sample_rate: float,\n",
    "    train_tensor: any,\n",
    "    test_tensor: any\n",
    ") -> any:\n",
    "    train_loader = DataLoader(\n",
    "        train_tensor,\n",
    "        batch_size=int(len(train_tensor) * sample_rate),\n",
    "        generator=torch.Generator().manual_seed(set_seed)\n",
    "    )\n",
    "    test_loader = DataLoader(test_tensor, 64)\n",
    "    return train_loader,test_loader\n",
    "\n",
    "def train(\n",
    "    model: any, \n",
    "    train_loader: any, \n",
    "    opt_func: any, \n",
    "    learning_rate: float, \n",
    "    num_epochs: int,  \n",
    "    random_seed: int, \n",
    "    verbose = True\n",
    ") -> int:\n",
    "    optimizer = opt_func(model.parameters(), learning_rate)\n",
    "    model_type = type(model)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model_type.train_step(model, batch)\n",
    "            loss.backward()\n",
    "            losses.append(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Epoch {}, loss = {}\".format(epoch + 1, torch.sum(loss) / len(train_loader)))\n",
    "   \n",
    "def test(\n",
    "    model: any, \n",
    "    test_loader: any\n",
    ") -> any:\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        total_size = 0\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            total_size += len(batch[1])\n",
    "            loss, corrects = model.test_step(model, batch)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(corrects)\n",
    "\n",
    "        average_loss = np.array(loss).sum() / total_size\n",
    "        total_accuracy = np.array(accuracies).sum() / total_size\n",
    "        return average_loss, total_accuracy\n",
    "\n",
    "def federated_model_pipeline(\n",
    "    given_parameters: any,\n",
    "    set_seed: int,\n",
    "    learning_rate: float,\n",
    "    sample_rate: float,\n",
    "    num_epochs: int,\n",
    "    input_dim: int,\n",
    "    train_tensor: any,\n",
    "    test_tensor: any\n",
    ") -> any:\n",
    "    torch.manual_seed(set_seed)\n",
    "    print('Loaders')\n",
    "    given_train_loader, given_test_loader = get_loaders(\n",
    "        set_seed,\n",
    "        sample_rate,\n",
    "        train_tensor,\n",
    "        test_tensor\n",
    "    )\n",
    "    print('Fed Model')\n",
    "    lr_model = FederatedLogisticRegression(dim = input_dim)\n",
    "    if given_parameters: \n",
    "        lr_model.apply_parameters(lr_model,given_parameters)\n",
    "    print('Train')\n",
    "    train(\n",
    "        model = lr_model, \n",
    "        train_loader = given_train_loader, \n",
    "        opt_func = torch.optim.SGD, \n",
    "        learning_rate = learning_rate, \n",
    "        num_epochs = num_epochs,  \n",
    "        random_seed = set_seed, \n",
    "        verbose = True\n",
    "    )\n",
    "    \n",
    "    print('Test')\n",
    "    average_loss, total_accuracy = test(\n",
    "        model = lr_model, \n",
    "        test_loader = given_test_loader\n",
    "    )\n",
    "    print('Complete')\n",
    "\n",
    "    parameters = lr_model.get_parameters(lr_model)\n",
    "    return average_loss, total_accuracy, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda08206-50bc-4d21-a6d2-c2cba085919e",
   "metadata": {},
   "source": [
    "### Central Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3caa1250-807a-412d-9b80-c9e1379b0759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (8000, 6)\n",
      "X test: (2000, 6)\n",
      "Y train: (8000,)\n",
      "Y test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "central_sample_size_1, input_dim, central_train_tensor_1, central_test_tensor_1 = preprocess_into_train_and_test_tensors(\n",
    "    data_path = 'data/Formated_Fraud_Detection_Data.csv',\n",
    "    used_columns = [\n",
    "        'amount',\n",
    "        'type_CASH_IN',\n",
    "        'type_CASH_OUT',\n",
    "        'type_DEBIT',\n",
    "        'type_PAYMENT',\n",
    "        'type_TRANSFER',\n",
    "        'isFraud'\n",
    "    ],\n",
    "    start_row = 0,\n",
    "    end_row = 10000,\n",
    "    scaled_columns = [\n",
    "        'amount'\n",
    "    ],\n",
    "    target_column = 'isFraud',\n",
    "    set_seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6779cf29-79c4-4b8e-9ca9-7a7baf2da8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders\n",
      "Fed Model\n",
      "Train\n",
      "Epoch 1, loss = 0.005733905825763941\n",
      "Epoch 2, loss = 0.005491575691848993\n",
      "Epoch 3, loss = 0.005264477338641882\n",
      "Epoch 4, loss = 0.005051491782069206\n",
      "Epoch 5, loss = 0.004851583391427994\n",
      "Test\n",
      "Complete\n",
      "0.00022987823188304901\n",
      "0.908\n",
      "OrderedDict([('linear.weight', tensor([[ 0.2859,  0.2883, -0.1216,  0.3666, -0.1894,  0.0594]])), ('linear.bias', tensor([-0.4066]))])\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, global_model_1 = federated_model_pipeline(\n",
    "    given_parameters = None,\n",
    "    set_seed = 42,\n",
    "    learning_rate = 0.001,\n",
    "    sample_rate = 0.01,\n",
    "    num_epochs = 5,\n",
    "    input_dim = input_dim,\n",
    "    train_tensor = central_train_tensor_1,\n",
    "    test_tensor = central_test_tensor_1\n",
    ")\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "print(global_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282705cb-a693-42d5-aa2c-8d2e86a0dcc7",
   "metadata": {},
   "source": [
    "## Worker 1 Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0750421b-8caa-4b2b-ae8d-3d052977ceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (8000, 6)\n",
      "X test: (2000, 6)\n",
      "Y train: (8000,)\n",
      "Y test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "worker_1_sample_size_1, input_dim, worker_1_train_tensor_1, worker_1_test_tensor_1 = preprocess_into_train_and_test_tensors(\n",
    "    data_path = 'data/Formated_Fraud_Detection_Data.csv',\n",
    "    used_columns = [\n",
    "        'amount',\n",
    "        'type_CASH_IN',\n",
    "        'type_CASH_OUT',\n",
    "        'type_DEBIT',\n",
    "        'type_PAYMENT',\n",
    "        'type_TRANSFER',\n",
    "        'isFraud'\n",
    "    ],\n",
    "    start_row = 10000,\n",
    "    end_row = 20000,\n",
    "    scaled_columns = [\n",
    "        'amount'\n",
    "    ],\n",
    "    target_column = 'isFraud',\n",
    "    set_seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6152a60a-c39f-444e-89dd-f52ace19fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders\n",
      "Fed Model\n",
      "Train\n",
      "Epoch 1, loss = 0.004760765470564365\n",
      "Epoch 2, loss = 0.004586684051901102\n",
      "Epoch 3, loss = 0.004422419238835573\n",
      "Epoch 4, loss = 0.004267293494194746\n",
      "Epoch 5, loss = 0.004120686091482639\n",
      "Test\n",
      "Complete\n",
      "0.00020317628979682923\n",
      "0.958\n",
      "OrderedDict([('linear.weight', tensor([[ 0.2548,  0.2538, -0.1625,  0.3646, -0.2644,  0.0338]])), ('linear.bias', tensor([-0.5846]))])\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, worker_1_model_1 = federated_model_pipeline(\n",
    "    given_parameters = global_model_1,\n",
    "    set_seed = 42,\n",
    "    learning_rate = 0.001,\n",
    "    sample_rate = 0.01,\n",
    "    num_epochs = 5,\n",
    "    input_dim = input_dim,\n",
    "    train_tensor = worker_1_train_tensor_1,\n",
    "    test_tensor = worker_1_test_tensor_1\n",
    ")\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "print(worker_1_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea5cfa-31c7-4dd9-a0ea-70d597f239be",
   "metadata": {},
   "source": [
    "## Worker 2 Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e86fe3b2-24f1-4312-9cac-9c1976e2b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (8000, 6)\n",
      "X test: (2000, 6)\n",
      "Y train: (8000,)\n",
      "Y test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "worker_2_sample_size_1,input_dim, worker_2_train_tensor_1, worker_2_test_tensor_1 = preprocess_into_train_and_test_tensors(\n",
    "    data_path = 'data/Formated_Fraud_Detection_Data.csv',\n",
    "    used_columns = [\n",
    "        'amount',\n",
    "        'type_CASH_IN',\n",
    "        'type_CASH_OUT',\n",
    "        'type_DEBIT',\n",
    "        'type_PAYMENT',\n",
    "        'type_TRANSFER',\n",
    "        'isFraud'\n",
    "    ],\n",
    "    start_row = 20000,\n",
    "    end_row = 30000,\n",
    "    scaled_columns = [\n",
    "        'amount'\n",
    "    ],\n",
    "    target_column = 'isFraud',\n",
    "    set_seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f01f5850-ca29-4988-9fcd-5f50858bd014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders\n",
      "Fed Model\n",
      "Train\n",
      "Epoch 1, loss = 0.004955528303980827\n",
      "Epoch 2, loss = 0.004771655425429344\n",
      "Epoch 3, loss = 0.004597960971295834\n",
      "Epoch 4, loss = 0.004433786496520042\n",
      "Epoch 5, loss = 0.004278519656509161\n",
      "Test\n",
      "Complete\n",
      "0.00018141770362854005\n",
      "0.9575\n",
      "OrderedDict([('linear.weight', tensor([[ 0.2526,  0.2437, -0.1726,  0.3647, -0.2506,  0.0369]])), ('linear.bias', tensor([-0.5878]))])\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, worker_2_model_1 = federated_model_pipeline(\n",
    "    given_parameters = global_model_1,\n",
    "    set_seed = 42,\n",
    "    learning_rate = 0.001,\n",
    "    sample_rate = 0.01,\n",
    "    num_epochs = 5,\n",
    "    input_dim = input_dim,\n",
    "    train_tensor = worker_2_train_tensor_1,\n",
    "    test_tensor = worker_2_test_tensor_1\n",
    ")\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "print(worker_2_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3c519-a330-49bd-8cd5-87c572bb136f",
   "metadata": {},
   "source": [
    "### Central FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a38a5f4-9142-4b3a-8206-235193e2c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n",
      "16000\n",
      "[ 0.254816    0.2537528  -0.16251254  0.36463702 -0.26442271  0.03384   ] -0.5846246480941772\n",
      "[ 0.25262707  0.2436935  -0.17256512  0.36465079 -0.25056174  0.0369154 ] -0.5877873301506042\n",
      "[array([ 0.25372154,  0.24872315, -0.16753883,  0.3646439 , -0.25749223,\n",
      "        0.0353777 ])] [-0.5862059891223907]\n",
      "OrderedDict([('linear.weight', tensor([[ 0.2537,  0.2487, -0.1675,  0.3646, -0.2575,  0.0354]])), ('linear.bias', tensor([-0.5862]))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6293/3936065571.py:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  ('linear.weight', torch.tensor(FedAvg_weight,dtype=torch.float32)),\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "received_updates = [\n",
    "    {'parameters':worker_1_model_1, 'samples': worker_1_sample_size_1},\n",
    "    {'parameters':worker_2_model_1, 'samples': worker_2_sample_size_1}\n",
    "]\n",
    "\n",
    "collective_sample_size = 0\n",
    "for update in received_updates:\n",
    "    print(update['samples'])\n",
    "    collective_sample_size += update['samples']\n",
    "    \n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "print(collective_sample_size)\n",
    "for update in received_updates:\n",
    "    parameters = update['parameters']\n",
    "    worker_sample_size = update['samples']\n",
    "    worker_weights = np.array(parameters['linear.weight'].tolist()[0])\n",
    "    worker_bias = parameters['linear.bias'].tolist()[0]\n",
    "    print(worker_weights,worker_bias)\n",
    "\n",
    "    adjusted_worker_weights = worker_weights * (worker_sample_size/collective_sample_size)\n",
    "    adjusted_worker_bias = worker_bias * (worker_sample_size/collective_sample_size)\n",
    "    \n",
    "    weights.append(adjusted_worker_weights.tolist())\n",
    "    biases.append(adjusted_worker_bias)\n",
    "\n",
    "weights = np.array(weights)\n",
    "biases = np.array(biases)\n",
    "\n",
    "FedAvg_weight = [np.sum(weights,axis = 0)]\n",
    "FedAvg_bias = [np.sum(biases, axis = 0)]\n",
    "\n",
    "print(FedAvg_weight,FedAvg_bias)\n",
    "\n",
    "global_model_2 = OrderedDict([\n",
    "    ('linear.weight', torch.tensor(FedAvg_weight,dtype=torch.float32)),\n",
    "    ('linear.bias', torch.tensor(FedAvg_bias,dtype=torch.float32))\n",
    "])\n",
    "print(global_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4923a-b928-4a94-be53-296743b10f47",
   "metadata": {},
   "source": [
    "### Global Model Evaluation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d392481c-7fe9-4c09-98ef-980994f6d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def preprocess_into_evaluation_tensor(\n",
    "    data_path: str,\n",
    "    used_columns: list,\n",
    "    start_row: int,\n",
    "    end_row: int,\n",
    "    scaled_columns: list,\n",
    "    target_column: str,\n",
    "    set_seed: int\n",
    ") -> any:\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    preprocessed_df = df[used_columns]\n",
    "\n",
    "    preprocessed_df = preprocessed_df[start_row:end_row]\n",
    "\n",
    "    for column in scaled_columns:\n",
    "        mean = preprocessed_df[column].mean()\n",
    "        std_dev = preprocessed_df[column].std()\n",
    "        preprocessed_df[column] = (preprocessed_df[column] - mean)/std_dev\n",
    "\n",
    "    X_test = preprocessed_df.drop(target_column, axis = 1).values\n",
    "    y_test = preprocessed_df[target_column].values\n",
    "        \n",
    "    print('X test:',X_test.shape)\n",
    "    print('Y test:',y_test.shape)\n",
    "\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_test = np.array(y_test, dtype=np.int32)\n",
    "    \n",
    "    test_tensor = TensorDataset(\n",
    "        torch.tensor(X_test), \n",
    "        torch.tensor(y_test, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "    return X_test.shape[0], X_test.shape[1], test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e58528b-70c5-4156-a434-6a8ff428e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedLogisticRegression(nn.Module):\n",
    "    def __init__(self, dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.linear = nn.Linear(dim, 1, bias=bias)\n",
    "        self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x).view(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def train_step(model, batch):\n",
    "        x, y = batch\n",
    "        out = model(x)\n",
    "        loss = model.loss(out, y)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def test_step(model, batch):\n",
    "        x, y = batch\n",
    "        out = model(x)\n",
    "        loss = model.loss(out, y)\n",
    "        preds = out > 0 # Predict y = 1 if P(y = 1) > 0.5\n",
    "        corrects = torch.tensor(torch.sum(preds == y).item())\n",
    "        return loss, corrects\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parameters(model):\n",
    "        return model.state_dict()\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_parameters(model, parameters):\n",
    "        model.load_state_dict(parameters)\n",
    "\n",
    "def test(\n",
    "    model: any, \n",
    "    test_loader: any\n",
    ") -> any:\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        total_size = 0\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            total_size += len(batch[1])\n",
    "            loss, corrects = model.test_step(model, batch)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(corrects)\n",
    "\n",
    "        average_loss = np.array(loss).sum() / total_size\n",
    "        total_accuracy = np.array(accuracies).sum() / total_size\n",
    "        return average_loss, total_accuracy\n",
    "\n",
    "def federated_model_evaluation(\n",
    "    given_parameters: any,\n",
    "    set_seed: int,\n",
    "    input_dim: int,\n",
    "    evaluation_tensor: any\n",
    ") -> any:\n",
    "    torch.manual_seed(set_seed)\n",
    "    print('Loader')\n",
    "    given_evaluation_loader = DataLoader(evaluation_tensor, 64)\n",
    "    \n",
    "    print('Fed Model')\n",
    "    lr_model = FederatedLogisticRegression(dim = input_dim)\n",
    "    lr_model.apply_parameters(lr_model,given_parameters)\n",
    "    \n",
    "    print('Test')\n",
    "    average_loss, total_accuracy = test(\n",
    "        model = lr_model, \n",
    "        test_loader = given_evaluation_loader\n",
    "    )\n",
    "    print('Complete')\n",
    "    return average_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39041173-d887-46d0-8fb1-c4527412299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test: (10000, 6)\n",
      "Y test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "evaluation_sample_size_1, input_dim, evaluation_tensor_1 = preprocess_into_evaluation_tensor(\n",
    "    data_path = 'data/Formated_Fraud_Detection_Data.csv',\n",
    "    used_columns = [\n",
    "        'amount',\n",
    "        'type_CASH_IN',\n",
    "        'type_CASH_OUT',\n",
    "        'type_DEBIT',\n",
    "        'type_PAYMENT',\n",
    "        'type_TRANSFER',\n",
    "        'isFraud'\n",
    "    ],\n",
    "    start_row = 30000,\n",
    "    end_row = 40000,\n",
    "    scaled_columns = [\n",
    "        'amount'\n",
    "    ],\n",
    "    target_column = 'isFraud',\n",
    "    set_seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78d48ff3-59b0-4aea-afeb-af5cd0d1b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader\n",
      "Fed Model\n",
      "Test\n",
      "Complete\n",
      "4.322848916053772e-05\n",
      "0.9636\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = federated_model_evaluation(\n",
    "    given_parameters = global_model_2,\n",
    "    set_seed = 42,\n",
    "    input_dim = input_dim,\n",
    "    evaluation_tensor = evaluation_tensor_1\n",
    ")\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1151d-af5b-42e7-aed6-c451c07eb0a9",
   "metadata": {},
   "source": [
    "### Worker node 1 Reupdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc51aff2-62cd-443e-bdff-6efe203497f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (8000, 6)\n",
      "X test: (2000, 6)\n",
      "Y train: (8000,)\n",
      "Y test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "worker_1_sample_size_2, input_dim, worker_1_train_tensor_2, worker_1_test_tensor_2 = preprocess_into_train_and_test_tensors(\n",
    "    data_path = 'data/Formated_Fraud_Detection_Data.csv',\n",
    "    used_columns = [\n",
    "        'amount',\n",
    "        'type_CASH_IN',\n",
    "        'type_CASH_OUT',\n",
    "        'type_DEBIT',\n",
    "        'type_PAYMENT',\n",
    "        'type_TRANSFER',\n",
    "        'isFraud'\n",
    "    ],\n",
    "    start_row = 40000,\n",
    "    end_row = 50000,\n",
    "    scaled_columns = [\n",
    "        'amount'\n",
    "    ],\n",
    "    target_column = 'isFraud',\n",
    "    set_seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "789c20a8-3231-4a45-85fe-63af297da6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders\n",
      "Fed Model\n",
      "Train\n",
      "Epoch 1, loss = 0.00407393230125308\n",
      "Epoch 2, loss = 0.003936012275516987\n",
      "Epoch 3, loss = 0.003805415239185095\n",
      "Epoch 4, loss = 0.003681669943034649\n",
      "Epoch 5, loss = 0.0035643333103507757\n",
      "Test\n",
      "Complete\n",
      "0.0001800934076309204\n",
      "0.98\n",
      "OrderedDict([('linear.weight', tensor([[ 0.2236,  0.2144, -0.2236,  0.3635, -0.3020,  0.0143]])), ('linear.bias', tensor([-0.7433]))])\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, worker_1_model_2 = federated_model_pipeline(\n",
    "    given_parameters = global_model_2,\n",
    "    set_seed = 42,\n",
    "    learning_rate = 0.001,\n",
    "    sample_rate = 0.01,\n",
    "    num_epochs = 5,\n",
    "    input_dim = input_dim,\n",
    "    train_tensor = worker_1_train_tensor_2,\n",
    "    test_tensor = worker_1_test_tensor_2\n",
    ")\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "print(worker_1_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eaba89-58e0-4c8d-965d-58888f70eddb",
   "metadata": {},
   "source": [
    "### Worker Node 2 Reupdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd01afec-2d40-44ed-ba91-1f55021e8d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (8000, 6)\n",
      "X test: (2000, 6)\n",
      "Y train: (8000,)\n",
      "Y test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "worker_2_sample_size_2,input_dim, worker_2_train_tensor_2, worker_2_test_tensor_2 = preprocess_into_train_and_test_tensors(\n",
    "    data_path = 'data/Formated_Fraud_Detection_Data.csv',\n",
    "    used_columns = [\n",
    "        'amount',\n",
    "        'type_CASH_IN',\n",
    "        'type_CASH_OUT',\n",
    "        'type_DEBIT',\n",
    "        'type_PAYMENT',\n",
    "        'type_TRANSFER',\n",
    "        'isFraud'\n",
    "    ],\n",
    "    start_row = 50000,\n",
    "    end_row = 60000,\n",
    "    scaled_columns = [\n",
    "        'amount'\n",
    "    ],\n",
    "    target_column = 'isFraud',\n",
    "    set_seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ebb2505-f47b-4da9-b206-233fdaf63c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders\n",
      "Fed Model\n",
      "Train\n",
      "Epoch 1, loss = 0.0038810980040580034\n",
      "Epoch 2, loss = 0.0037517123855650425\n",
      "Epoch 3, loss = 0.0036292257718741894\n",
      "Epoch 4, loss = 0.0035131804179400206\n",
      "Epoch 5, loss = 0.003403153968974948\n",
      "Test\n",
      "Complete\n",
      "0.00017841057479381562\n",
      "0.971\n",
      "OrderedDict([('linear.weight', tensor([[ 0.2244,  0.2080, -0.2197,  0.3632, -0.3028,  0.0167]])), ('linear.bias', tensor([-0.7445]))])\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, worker_2_model_2 = federated_model_pipeline(\n",
    "    given_parameters = global_model_2,\n",
    "    set_seed = 42,\n",
    "    learning_rate = 0.001,\n",
    "    sample_rate = 0.01,\n",
    "    num_epochs = 5,\n",
    "    input_dim = input_dim,\n",
    "    train_tensor = worker_2_train_tensor_2,\n",
    "    test_tensor = worker_2_test_tensor_2\n",
    ")\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "print(worker_2_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788b827-e020-4609-a9e3-d76b713bfb50",
   "metadata": {},
   "source": [
    "## Final Central FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8d9ed67-9426-4c63-81e2-e06def39f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n",
      "16000\n",
      "[ 0.22358397  0.21440691 -0.22364609  0.36348364 -0.30195028  0.01430325] -0.7433218955993652\n",
      "[ 0.22438714  0.2079628  -0.21968411  0.36321157 -0.30277276  0.01668498] -0.7445172071456909\n",
      "[array([ 0.22398555,  0.21118485, -0.2216651 ,  0.3633476 , -0.30236152,\n",
      "        0.01549411])] [-0.7439195513725281]\n",
      "OrderedDict([('linear.weight', tensor([[ 0.2240,  0.2112, -0.2217,  0.3633, -0.3024,  0.0155]])), ('linear.bias', tensor([-0.7439]))])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "received_updates = [\n",
    "    {'parameters':worker_1_model_2, 'samples': worker_1_sample_size_2},\n",
    "    {'parameters':worker_2_model_2, 'samples': worker_2_sample_size_2}\n",
    "]\n",
    "\n",
    "collective_sample_size = 0\n",
    "for update in received_updates:\n",
    "    print(update['samples'])\n",
    "    collective_sample_size += update['samples']\n",
    "    \n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "print(collective_sample_size)\n",
    "for update in received_updates:\n",
    "    parameters = update['parameters']\n",
    "    worker_sample_size = update['samples']\n",
    "    worker_weights = np.array(parameters['linear.weight'].tolist()[0])\n",
    "    worker_bias = parameters['linear.bias'].tolist()[0]\n",
    "    print(worker_weights,worker_bias)\n",
    "\n",
    "    adjusted_worker_weights = worker_weights * (worker_sample_size/collective_sample_size)\n",
    "    adjusted_worker_bias = worker_bias * (worker_sample_size/collective_sample_size)\n",
    "    \n",
    "    weights.append(adjusted_worker_weights.tolist())\n",
    "    biases.append(adjusted_worker_bias)\n",
    "\n",
    "weights = np.array(weights)\n",
    "biases = np.array(biases)\n",
    "\n",
    "FedAvg_weight = [np.sum(weights,axis = 0)]\n",
    "FedAvg_bias = [np.sum(biases, axis = 0)]\n",
    "\n",
    "print(FedAvg_weight,FedAvg_bias)\n",
    "\n",
    "global_model_3 = OrderedDict([\n",
    "    ('linear.weight', torch.tensor(FedAvg_weight,dtype=torch.float32)),\n",
    "    ('linear.bias', torch.tensor(FedAvg_bias,dtype=torch.float32))\n",
    "])\n",
    "print(global_model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865e9d1-f871-4fe8-8699-4078450ab201",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "442c5398-6954-41f7-b604-1a89e21212e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test: (10000, 6)\n",
      "Y test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "evaluation_sample_size_2, input_dim, evaluation_tensor_2 = preprocess_into_evaluation_tensor(\n",
    "    data_path = 'data/Formated_Fraud_Detection_Data.csv',\n",
    "    used_columns = [\n",
    "        'amount',\n",
    "        'type_CASH_IN',\n",
    "        'type_CASH_OUT',\n",
    "        'type_DEBIT',\n",
    "        'type_PAYMENT',\n",
    "        'type_TRANSFER',\n",
    "        'isFraud'\n",
    "    ],\n",
    "    start_row = 60000,\n",
    "    end_row = 70000,\n",
    "    scaled_columns = [\n",
    "        'amount'\n",
    "    ],\n",
    "    target_column = 'isFraud',\n",
    "    set_seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcd8c6f1-ef26-4d3c-bc70-2803aa6b84de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader\n",
      "Fed Model\n",
      "Test\n",
      "Complete\n",
      "3.361541330814361e-05\n",
      "0.9787\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = federated_model_evaluation(\n",
    "    given_parameters = global_model_3,\n",
    "    set_seed = 42,\n",
    "    input_dim = input_dim,\n",
    "    evaluation_tensor = evaluation_tensor_2\n",
    ")\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad885c-e3cb-480f-868f-15f7375d9596",
   "metadata": {},
   "source": [
    "## Central and Worker ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c4682-a19c-4958-ae6b-8191c00ad291",
   "metadata": {},
   "source": [
    "By studying Federeated Learning with Pytorch, we conclude that the central and worker requires the following functions, background jobs and routes:\n",
    "\n",
    "central:\n",
    "- functions:\n",
    "    - format_data (optional) \n",
    "    - preprocess_data \n",
    "    - initilize_global_model\n",
    "    - evaluate_global_model\n",
    "    - list_workers\n",
    "    - fed_avg\n",
    "    - update_global_model\n",
    "    - prepare_worker_data\n",
    "    - send_worker_data\n",
    "- background:\n",
    "    - create/check_workers\n",
    "    - check_updates\n",
    "    - collect_model_metrics\n",
    "- routes:\n",
    "    - receive_update\n",
    "    - get_central_logs\n",
    "    - inference\n",
    "    - start_learning\n",
    "\n",
    "worker:\n",
    "- functions:\n",
    "    - preprocess_data\n",
    "    - set_local_model\n",
    "    - evaluate_local_model\n",
    "- background:\n",
    "    - train_local_model\n",
    "    - collect_model_metrics\n",
    "- routes:\n",
    "    - receive_configuration \n",
    "    - receive_worker_data \n",
    "    - receive_global_model\n",
    "    - get_worker_logs\n",
    "    - inference\n",
    "    - start_learning      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d67bc31-8a7e-43f1-a246-10c96ffe5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c11365-6992-4456-a275-7633abbab2a1",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4151c86-daa4-4077-8a09-301cf27a6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedLogisticRegression(nn.Module):\n",
    "    def __init__(self, dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.linear = nn.Linear(dim, 1, bias=bias)\n",
    "        self.loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x).view(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def train_step(model, batch):\n",
    "        x, y = batch\n",
    "        out = model(x)\n",
    "        loss = model.loss(out, y)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def test_step(model, batch):\n",
    "        x, y = batch\n",
    "        out = model(x)\n",
    "        loss = model.loss(out, y)\n",
    "        preds = out > 0 # Predict y = 1 if P(y = 1) > 0.5\n",
    "        corrects = torch.tensor(torch.sum(preds == y).item())\n",
    "        return loss, corrects\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parameters(model):\n",
    "        return model.state_dict()\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_parameters(model, parameters):\n",
    "        model.load_state_dict(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821b92a0-695b-4692-99a2-0e3ac7b0f92a",
   "metadata": {},
   "source": [
    "### Central Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689275d-a5e3-4718-b982-c916a8678269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "909bdb47-4ccf-4017-9e58-4ae7214d22cb",
   "metadata": {},
   "source": [
    "### Worker Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a9673-569c-47b3-8cf4-ff27bfac74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_into_train_and_test_tensors(\n",
    "    data_path: str,\n",
    "    used_columns: list,\n",
    "    start_row: int,\n",
    "    end_row: int,\n",
    "    scaled_columns: list,\n",
    "    target_column: str,\n",
    "    set_seed: int\n",
    ") -> any:\n",
    "    np.random.seed(set_seed)\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    preprocessed_df = df[used_columns]\n",
    "\n",
    "    preprocessed_df = preprocessed_df[start_row:end_row]\n",
    "\n",
    "    for column in scaled_columns:\n",
    "        mean = preprocessed_df[column].mean()\n",
    "        std_dev = preprocessed_df[column].std()\n",
    "        preprocessed_df[column] = (preprocessed_df[column] - mean)/std_dev\n",
    "\n",
    "    X = preprocessed_df.drop(target_column, axis = 1).values\n",
    "    y = preprocessed_df[target_column].values\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, \n",
    "        y, \n",
    "        test_size = 0.2, \n",
    "        random_state = set_seed\n",
    "    )\n",
    "\n",
    "    print('X train:',X_train.shape)\n",
    "    print('X test:',X_test.shape)\n",
    "    print('Y train:',y_train.shape)\n",
    "    print('Y test:',y_test.shape)\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.int32)\n",
    "    y_test = np.array(y_test, dtype=np.int32)\n",
    "    \n",
    "    train_tensor = TensorDataset(\n",
    "        torch.tensor(X_train), \n",
    "        torch.tensor(y_train, dtype=torch.float32)\n",
    "    )\n",
    "    test_tensor = TensorDataset(\n",
    "        torch.tensor(X_test), \n",
    "        torch.tensor(y_test, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "    return X_train.shape[0], X_train.shape[1], train_tensor, test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2dbc7b-88e5-45aa-ad11-8d50b9994a4e",
   "metadata": {},
   "source": [
    "Used imports are:\n",
    "- pip install pandas\n",
    "- pip install numpy\n",
    "- pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad8761-2a77-4b57-90d3-abaaac553767",
   "metadata": {},
   "source": [
    "## Worker ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39eaa3-0b0c-43d6-a8ba-b0fb43922af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
